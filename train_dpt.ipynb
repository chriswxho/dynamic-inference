{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc8bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import util.io\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from dpt.models import DPTDepthModel\n",
    "from dpt.midas_net import MidasNet_large\n",
    "from dpt.transforms import Resize, NormalizeImage, PrepareForNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec77edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# k8s paths\n",
    "k8s = False\n",
    "k8s_repo = r'opt/repo/dynamic-inference'\n",
    "k8s_pvc = r'../../christh9-pvc'\n",
    "\n",
    "# path settings\n",
    "input_path = 'input'\n",
    "output_path = 'output_monodepth'\n",
    "model_path = 'weights/dpt_hybrid_nyu-2ce69ec7.pt'\n",
    "\n",
    "if k8s:\n",
    "    input_path = os.path.join(k8s_repo, input_path)\n",
    "    output_path = os.path.join(k8s_repo, output_path)\n",
    "    model_path = os.path.join(k8s_pvc, 'dpt-hybrid-nyu.pt')\n",
    "#     script_output = os.path.join(k8s_pvc, 'dpt-timings', f'runtimes-{device_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2361ebfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPTDepthModel(\n",
       "  (pretrained): Module(\n",
       "    (model): VisionTransformer(\n",
       "      (patch_embed): HybridEmbed(\n",
       "        (backbone): ResNetV2(\n",
       "          (stem): Sequential(\n",
       "            (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "            (norm): GroupNormAct(\n",
       "              32, 64, eps=1e-05, affine=True\n",
       "              (act): ReLU(inplace=True)\n",
       "            )\n",
       "            (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=True)\n",
       "          )\n",
       "          (stages): Sequential(\n",
       "            (0): ResNetStage(\n",
       "              (blocks): Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (downsample): DownsampleConv(\n",
       "                    (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (norm): GroupNormAct(\n",
       "                      32, 256, eps=1e-05, affine=True\n",
       "                      (act): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 64, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ResNetStage(\n",
       "              (blocks): Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (downsample): DownsampleConv(\n",
       "                    (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (norm): GroupNormAct(\n",
       "                      32, 512, eps=1e-05, affine=True\n",
       "                      (act): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 512, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 512, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 512, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (3): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 128, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 512, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ResNetStage(\n",
       "              (blocks): Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (downsample): DownsampleConv(\n",
       "                    (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (norm): GroupNormAct(\n",
       "                      32, 1024, eps=1e-05, affine=True\n",
       "                      (act): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (3): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (4): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (5): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (6): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (7): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "                (8): Bottleneck(\n",
       "                  (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm1): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (norm2): GroupNormAct(\n",
       "                    32, 256, eps=1e-05, affine=True\n",
       "                    (act): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (norm3): GroupNormAct(\n",
       "                    32, 1024, eps=1e-05, affine=True\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (act3): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): Identity()\n",
       "          (head): ClassifierHead(\n",
       "            (global_pool): SelectAdaptivePool2d (pool_type=, flatten=False)\n",
       "            (fc): Identity()\n",
       "          )\n",
       "        )\n",
       "        (proj): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "    (act_postprocess1): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (act_postprocess2): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (act_postprocess3): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "          (1): GELU()\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (act_postprocess4): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "          (1): GELU()\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (scratch): Module(\n",
       "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer3_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer4_rn): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (refinenet1): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet2): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet3): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet4): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_conv): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Interpolate()\n",
       "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: %s\" % device)\n",
    "\n",
    "model_type = 'dpt_hybrid_nyu'\n",
    "\n",
    "# load network\n",
    "if model_type == \"dpt_large\":  # DPT-Large\n",
    "    net_w = net_h = 384\n",
    "    model = DPTDepthModel(\n",
    "        path=model_path,\n",
    "        backbone=\"vitl16_384\",\n",
    "        non_negative=True,\n",
    "        enable_attention_hooks=False,\n",
    "    )\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "elif model_type == \"dpt_hybrid\":  # DPT-Hybrid\n",
    "    net_w = net_h = 384\n",
    "    model = DPTDepthModel(\n",
    "        path=model_path,\n",
    "        backbone=\"vitb_rn50_384\",\n",
    "        non_negative=True,\n",
    "        enable_attention_hooks=False,\n",
    "    )\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "elif model_type == \"dpt_hybrid_kitti\":\n",
    "    net_w = 1216\n",
    "    net_h = 352\n",
    "\n",
    "    model = DPTDepthModel(\n",
    "        path=model_path,\n",
    "        scale=0.00006016,\n",
    "        shift=0.00579,\n",
    "        invert=True,\n",
    "        backbone=\"vitb_rn50_384\",\n",
    "        non_negative=True,\n",
    "        enable_attention_hooks=False,\n",
    "    )\n",
    "\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "elif model_type == \"dpt_hybrid_nyu\":\n",
    "    net_w = 640\n",
    "    net_h = 480\n",
    "\n",
    "    model = DPTDepthModel(\n",
    "        path=model_path,\n",
    "        scale=0.000305,\n",
    "        shift=0.1378,\n",
    "        invert=True,\n",
    "        backbone=\"vitb_rn50_384\",\n",
    "        non_negative=True,\n",
    "        enable_attention_hooks=False,\n",
    "    )\n",
    "\n",
    "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "elif model_type == \"midas_v21\":  # Convolutional model\n",
    "    net_w = net_h = 384\n",
    "\n",
    "    model = MidasNet_large(model_path, non_negative=True)\n",
    "    normalization = NormalizeImage(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "else:\n",
    "    assert (\n",
    "        False\n",
    "    ), f\"model_type '{model_type}' not implemented, use: --model_type [dpt_large|dpt_hybrid|dpt_hybrid_kitti|dpt_hybrid_nyu|midas_v21]\"\n",
    "    \n",
    "transform = Compose(\n",
    "    [\n",
    "        Resize(\n",
    "            net_w,\n",
    "            net_h,\n",
    "            resize_target=None,\n",
    "            keep_aspect_ratio=True,\n",
    "            ensure_multiple_of=32,\n",
    "            resize_method=\"minimal\",\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        normalization,\n",
    "        PrepareForNet(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9db6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class InteriorNetDPT(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DPTDepthModel(\n",
    "                        path=model_path,\n",
    "#                         scale=s,\n",
    "#                         shift=t,\n",
    "                        scale=0.000305,\n",
    "                        shift=0.1378,\n",
    "                        invert=True,\n",
    "                        backbone=\"vitb_rn50_384\",\n",
    "                        non_negative=True,\n",
    "                        enable_attention_hooks=False,\n",
    "                     )\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['image'], batch['depth']\n",
    "        yhat = self.model(x)\n",
    "        loss = SILog(yhat, 1/y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        \n",
    "        metrics = get_metrics(yhat.detach(), 1/y.detach())\n",
    "        self.log('absrel', metrics[0], on_epoch=True)\n",
    "        self.log('delta_acc', metrics[1], on_epoch=True)\n",
    "        self.log('mae', metrics[2], on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "model = InteriorNetDPT()\n",
    "model.model.pretrained.model.patch_embed.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dd9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to implement this in training, don't use this for now\n",
    "\n",
    "# if optimize == True and device == torch.device(\"cuda\"):\n",
    "#     model = model.to(memory_format=torch.channels_last)\n",
    "#     model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99497224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from video_inference_common.video_inference.datasets import interiornet\n",
    "\n",
    "def getlines(files: [str], subsample):\n",
    "    ''' helper function to get stripped lines from multiple files'''\n",
    "    names = []\n",
    "    # todo: delete after verification\n",
    "    if not subsample:\n",
    "        for f in files:\n",
    "            names.append(map(lambda x: x.strip(), open(f).readlines()))\n",
    "        return list(itertools.chain.from_iterable(names))\n",
    "\n",
    "    else:\n",
    "        for f in files:\n",
    "            names.append(open(f).readline().strip())\n",
    "            break\n",
    "            \n",
    "        return names\n",
    "\n",
    "class InteriorNetDataset(Dataset):\n",
    "    def __init__(self, dataset_path: str, train=True, transform=None, subsample=False):\n",
    "        '''\n",
    "        dataset_path: path to the folder containing the txts that specify dataset\n",
    "                      (relative to ./dynamic-inference)\n",
    "        train: specify to use the training or test split\n",
    "        transform: optional transform to be applied per sample\n",
    "        subsample: take a subsample of all the training data\n",
    "        '''\n",
    "        subsets = re.compile(f'.*?({\"train\" if train else \"test\"}).*?')\n",
    "        video_names = map(lambda p: os.path.join(dataset_path, p), \n",
    "                          filter(subsets.match, os.listdir(dataset_path)))\n",
    "        self.videos = np.array(getlines(video_names, subsample))\n",
    "        self.transform = transform\n",
    "        self.path = dataset_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000 * len(self.videos) # each video is 1000 frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # idx will come as video_index * frame_index\n",
    "        img_name = self.videos[idx // 1000]\n",
    "        frame_idx = idx % 1000\n",
    "        \n",
    "        im = interiornet.read_rgb(img_name, frame_idx)\n",
    "        depth = interiornet.read_depth(img_name, frame_idx)\n",
    "        \n",
    "        if self.transform:\n",
    "            im = self.transform({'image': im})['image']\n",
    "        \n",
    "        return {'image': im, 'depth': depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46678367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SILogLoss(yhat, y, L = 1):\n",
    "    '''\n",
    "    yhat: prediction\n",
    "    y: ground truth\n",
    "    L:  in the paper, [0,1]. L=0 gives elementwise L2 loss, \n",
    "       L=1 gives scale-invariant loss.\n",
    "    https://arxiv.org/pdf/1406.2283.pdf\n",
    "    '''\n",
    "    idx = ~torch.isnan(y)\n",
    "    di = torch.log(yhat[idx]) - torch.log(y[idx])\n",
    "    \n",
    "    return (di**2).mean() - L * di.mean() ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300f7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(yhat, y, metrics=['absrel', 'delta', 'mae']):\n",
    "    \n",
    "    yhat.detach()\n",
    "    y.detach()\n",
    "    \n",
    "    values = []\n",
    "    idx = ~torch.isnan(y)\n",
    "    \n",
    "    if 'absrel' in metrics:\n",
    "        values.append((torch.abs(y[idx] - yhat[idx]) / yhat[idx]).mean())\n",
    "    if 'delta' in metrics:\n",
    "        values.append(0)\n",
    "    if 'mae' in metrics:\n",
    "        values.append((torch.abs(y[idx] - yhat[idx])).mean())\n",
    "        \n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45b9063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be9f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'video_inference_common/resources'\n",
    "if k8s:\n",
    "    dataset_path = os.path.join(k8s_repo, dataset_path)\n",
    "    \n",
    "batch_size = 1\n",
    "\n",
    "interiornet_dataset = InteriorNetDataset(dataset_path, transform=transform, subsample=True)\n",
    "dataloader = DataLoader(interiornet_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d4d1b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 0.07850567\n",
      "t: 2.8652542\n"
     ]
    }
   ],
   "source": [
    "# get shifted statistics\n",
    "full_dataset = InteriorNetDataset(dataset_path, transform=transform)\n",
    "d = []\n",
    "p = 0.1\n",
    "for i in np.random.choice(len(full_dataset), size=round(p*len(full_dataset)), replace=False):\n",
    "    d.append(full_dataset[i]['depth'].flatten())\n",
    "\n",
    "d = np.array(d)\n",
    "t = np.median(d)\n",
    "s = (d - t).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7523f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cho/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1\n",
    "losses = []\n",
    "metrics = []\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    running_loss = 0.0\n",
    "    for sample in dataloader:\n",
    "        x, y = sample['image'], sample['depth']\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "        yhat = model(x)\n",
    "        loss = SILogLoss((yhat - t) / s, y)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "        metrics.append(get_metrics(yhat, y))\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    losses.append(running_loss / len(interiornet_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data saving\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "\n",
    "logs_dir = os.path.join(k8s_pvc, 'train-logs')\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.mkdir(logs_dir)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "                   'loss': losses, \n",
    "                   'absrel': metrics[:,0],\n",
    "                   'mae': metrics[:,1],\n",
    "                   'delta': metrics[:,2]\n",
    "                  })\n",
    "\n",
    "df.to_csv(os.path.join(logs_dir, 'testrun.csv'))\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(logs_dir, 'finetune.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72f3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/Users/cho/.pyenv/versions/3.8.5/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c552a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1abd346d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cho/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     StdConv2dSame-1         [-1, 64, 240, 320]           9,408\n",
      "              ReLU-2         [-1, 64, 240, 320]               0\n",
      "      GroupNormAct-3         [-1, 64, 240, 320]             128\n",
      "     MaxPool2dSame-4         [-1, 64, 120, 160]               0\n",
      "     StdConv2dSame-5        [-1, 256, 120, 160]          16,384\n",
      "          Identity-6        [-1, 256, 120, 160]               0\n",
      "      GroupNormAct-7        [-1, 256, 120, 160]             512\n",
      "    DownsampleConv-8        [-1, 256, 120, 160]               0\n",
      "     StdConv2dSame-9         [-1, 64, 120, 160]           4,096\n",
      "             ReLU-10         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-11         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-12         [-1, 64, 120, 160]          36,864\n",
      "             ReLU-13         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-14         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-15        [-1, 256, 120, 160]          16,384\n",
      "         Identity-16        [-1, 256, 120, 160]               0\n",
      "     GroupNormAct-17        [-1, 256, 120, 160]             512\n",
      "         Identity-18        [-1, 256, 120, 160]               0\n",
      "             ReLU-19        [-1, 256, 120, 160]               0\n",
      "       Bottleneck-20        [-1, 256, 120, 160]               0\n",
      "    StdConv2dSame-21         [-1, 64, 120, 160]          16,384\n",
      "             ReLU-22         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-23         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-24         [-1, 64, 120, 160]          36,864\n",
      "             ReLU-25         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-26         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-27        [-1, 256, 120, 160]          16,384\n",
      "         Identity-28        [-1, 256, 120, 160]               0\n",
      "     GroupNormAct-29        [-1, 256, 120, 160]             512\n",
      "         Identity-30        [-1, 256, 120, 160]               0\n",
      "             ReLU-31        [-1, 256, 120, 160]               0\n",
      "       Bottleneck-32        [-1, 256, 120, 160]               0\n",
      "    StdConv2dSame-33         [-1, 64, 120, 160]          16,384\n",
      "             ReLU-34         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-35         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-36         [-1, 64, 120, 160]          36,864\n",
      "             ReLU-37         [-1, 64, 120, 160]               0\n",
      "     GroupNormAct-38         [-1, 64, 120, 160]             128\n",
      "    StdConv2dSame-39        [-1, 256, 120, 160]          16,384\n",
      "         Identity-40        [-1, 256, 120, 160]               0\n",
      "     GroupNormAct-41        [-1, 256, 120, 160]             512\n",
      "         Identity-42        [-1, 256, 120, 160]               0\n",
      "             ReLU-43        [-1, 256, 120, 160]               0\n",
      "       Bottleneck-44        [-1, 256, 120, 160]               0\n",
      "      ResNetStage-45        [-1, 256, 120, 160]               0\n",
      "    StdConv2dSame-46          [-1, 512, 60, 80]         131,072\n",
      "         Identity-47          [-1, 512, 60, 80]               0\n",
      "     GroupNormAct-48          [-1, 512, 60, 80]           1,024\n",
      "   DownsampleConv-49          [-1, 512, 60, 80]               0\n",
      "    StdConv2dSame-50        [-1, 128, 120, 160]          32,768\n",
      "             ReLU-51        [-1, 128, 120, 160]               0\n",
      "     GroupNormAct-52        [-1, 128, 120, 160]             256\n",
      "    StdConv2dSame-53          [-1, 128, 60, 80]         147,456\n",
      "             ReLU-54          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-55          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-56          [-1, 512, 60, 80]          65,536\n",
      "         Identity-57          [-1, 512, 60, 80]               0\n",
      "     GroupNormAct-58          [-1, 512, 60, 80]           1,024\n",
      "         Identity-59          [-1, 512, 60, 80]               0\n",
      "             ReLU-60          [-1, 512, 60, 80]               0\n",
      "       Bottleneck-61          [-1, 512, 60, 80]               0\n",
      "    StdConv2dSame-62          [-1, 128, 60, 80]          65,536\n",
      "             ReLU-63          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-64          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-65          [-1, 128, 60, 80]         147,456\n",
      "             ReLU-66          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-67          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-68          [-1, 512, 60, 80]          65,536\n",
      "         Identity-69          [-1, 512, 60, 80]               0\n",
      "     GroupNormAct-70          [-1, 512, 60, 80]           1,024\n",
      "         Identity-71          [-1, 512, 60, 80]               0\n",
      "             ReLU-72          [-1, 512, 60, 80]               0\n",
      "       Bottleneck-73          [-1, 512, 60, 80]               0\n",
      "    StdConv2dSame-74          [-1, 128, 60, 80]          65,536\n",
      "             ReLU-75          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-76          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-77          [-1, 128, 60, 80]         147,456\n",
      "             ReLU-78          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-79          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-80          [-1, 512, 60, 80]          65,536\n",
      "         Identity-81          [-1, 512, 60, 80]               0\n",
      "     GroupNormAct-82          [-1, 512, 60, 80]           1,024\n",
      "         Identity-83          [-1, 512, 60, 80]               0\n",
      "             ReLU-84          [-1, 512, 60, 80]               0\n",
      "       Bottleneck-85          [-1, 512, 60, 80]               0\n",
      "    StdConv2dSame-86          [-1, 128, 60, 80]          65,536\n",
      "             ReLU-87          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-88          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-89          [-1, 128, 60, 80]         147,456\n",
      "             ReLU-90          [-1, 128, 60, 80]               0\n",
      "     GroupNormAct-91          [-1, 128, 60, 80]             256\n",
      "    StdConv2dSame-92          [-1, 512, 60, 80]          65,536\n",
      "         Identity-93          [-1, 512, 60, 80]               0\n",
      "     GroupNormAct-94          [-1, 512, 60, 80]           1,024\n",
      "         Identity-95          [-1, 512, 60, 80]               0\n",
      "             ReLU-96          [-1, 512, 60, 80]               0\n",
      "       Bottleneck-97          [-1, 512, 60, 80]               0\n",
      "      ResNetStage-98          [-1, 512, 60, 80]               0\n",
      "    StdConv2dSame-99         [-1, 1024, 30, 40]         524,288\n",
      "        Identity-100         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-101         [-1, 1024, 30, 40]           2,048\n",
      "  DownsampleConv-102         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-103          [-1, 256, 60, 80]         131,072\n",
      "            ReLU-104          [-1, 256, 60, 80]               0\n",
      "    GroupNormAct-105          [-1, 256, 60, 80]             512\n",
      "   StdConv2dSame-106          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-107          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-108          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-109         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-110         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-111         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-112         [-1, 1024, 30, 40]               0\n",
      "            ReLU-113         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-114         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-115          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-116          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-117          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-118          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-119          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-120          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-121         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-122         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-123         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-124         [-1, 1024, 30, 40]               0\n",
      "            ReLU-125         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-126         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-127          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-128          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-129          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-130          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-131          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-132          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-133         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-134         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-135         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-136         [-1, 1024, 30, 40]               0\n",
      "            ReLU-137         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-138         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-139          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-140          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-141          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-142          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-143          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-144          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-145         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-146         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-147         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-148         [-1, 1024, 30, 40]               0\n",
      "            ReLU-149         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-150         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-151          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-152          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-153          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-154          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-155          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-156          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-157         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-158         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-159         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-160         [-1, 1024, 30, 40]               0\n",
      "            ReLU-161         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-162         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-163          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-164          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-165          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-166          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-167          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-168          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-169         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-170         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-171         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-172         [-1, 1024, 30, 40]               0\n",
      "            ReLU-173         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-174         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-175          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-176          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-177          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-178          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-179          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-180          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-181         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-182         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-183         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-184         [-1, 1024, 30, 40]               0\n",
      "            ReLU-185         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-186         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-187          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-188          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-189          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-190          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-191          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-192          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-193         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-194         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-195         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-196         [-1, 1024, 30, 40]               0\n",
      "            ReLU-197         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-198         [-1, 1024, 30, 40]               0\n",
      "   StdConv2dSame-199          [-1, 256, 30, 40]         262,144\n",
      "            ReLU-200          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-201          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-202          [-1, 256, 30, 40]         589,824\n",
      "            ReLU-203          [-1, 256, 30, 40]               0\n",
      "    GroupNormAct-204          [-1, 256, 30, 40]             512\n",
      "   StdConv2dSame-205         [-1, 1024, 30, 40]         262,144\n",
      "        Identity-206         [-1, 1024, 30, 40]               0\n",
      "    GroupNormAct-207         [-1, 1024, 30, 40]           2,048\n",
      "        Identity-208         [-1, 1024, 30, 40]               0\n",
      "            ReLU-209         [-1, 1024, 30, 40]               0\n",
      "      Bottleneck-210         [-1, 1024, 30, 40]               0\n",
      "     ResNetStage-211         [-1, 1024, 30, 40]               0\n",
      "        Identity-212         [-1, 1024, 30, 40]               0\n",
      "        Identity-213         [-1, 1024, 30, 40]               0\n",
      "SelectAdaptivePool2d-214         [-1, 1024, 30, 40]               0\n",
      "        Identity-215         [-1, 1024, 30, 40]               0\n",
      "  ClassifierHead-216         [-1, 1024, 30, 40]               0\n",
      "        ResNetV2-217         [-1, 1024, 30, 40]               0\n",
      "          Conv2d-218          [-1, 768, 30, 40]         787,200\n",
      "         Dropout-219            [-1, 1201, 768]               0\n",
      "       LayerNorm-220            [-1, 1201, 768]           1,536\n",
      "          Linear-221           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-222       [-1, 12, 1201, 1201]               0\n",
      "          Linear-223            [-1, 1201, 768]         590,592\n",
      "         Dropout-224            [-1, 1201, 768]               0\n",
      "       Attention-225            [-1, 1201, 768]               0\n",
      "        Identity-226            [-1, 1201, 768]               0\n",
      "       LayerNorm-227            [-1, 1201, 768]           1,536\n",
      "          Linear-228           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-229           [-1, 1201, 3072]               0\n",
      "         Dropout-230           [-1, 1201, 3072]               0\n",
      "          Linear-231            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-232            [-1, 1201, 768]               0\n",
      "             Mlp-233            [-1, 1201, 768]               0\n",
      "        Identity-234            [-1, 1201, 768]               0\n",
      "           Block-235            [-1, 1201, 768]               0\n",
      "       LayerNorm-236            [-1, 1201, 768]           1,536\n",
      "          Linear-237           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-238       [-1, 12, 1201, 1201]               0\n",
      "          Linear-239            [-1, 1201, 768]         590,592\n",
      "         Dropout-240            [-1, 1201, 768]               0\n",
      "       Attention-241            [-1, 1201, 768]               0\n",
      "        Identity-242            [-1, 1201, 768]               0\n",
      "       LayerNorm-243            [-1, 1201, 768]           1,536\n",
      "          Linear-244           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-245           [-1, 1201, 3072]               0\n",
      "         Dropout-246           [-1, 1201, 3072]               0\n",
      "          Linear-247            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-248            [-1, 1201, 768]               0\n",
      "             Mlp-249            [-1, 1201, 768]               0\n",
      "        Identity-250            [-1, 1201, 768]               0\n",
      "           Block-251            [-1, 1201, 768]               0\n",
      "       LayerNorm-252            [-1, 1201, 768]           1,536\n",
      "          Linear-253           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-254       [-1, 12, 1201, 1201]               0\n",
      "          Linear-255            [-1, 1201, 768]         590,592\n",
      "         Dropout-256            [-1, 1201, 768]               0\n",
      "       Attention-257            [-1, 1201, 768]               0\n",
      "        Identity-258            [-1, 1201, 768]               0\n",
      "       LayerNorm-259            [-1, 1201, 768]           1,536\n",
      "          Linear-260           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-261           [-1, 1201, 3072]               0\n",
      "         Dropout-262           [-1, 1201, 3072]               0\n",
      "          Linear-263            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-264            [-1, 1201, 768]               0\n",
      "             Mlp-265            [-1, 1201, 768]               0\n",
      "        Identity-266            [-1, 1201, 768]               0\n",
      "           Block-267            [-1, 1201, 768]               0\n",
      "       LayerNorm-268            [-1, 1201, 768]           1,536\n",
      "          Linear-269           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-270       [-1, 12, 1201, 1201]               0\n",
      "          Linear-271            [-1, 1201, 768]         590,592\n",
      "         Dropout-272            [-1, 1201, 768]               0\n",
      "       Attention-273            [-1, 1201, 768]               0\n",
      "        Identity-274            [-1, 1201, 768]               0\n",
      "       LayerNorm-275            [-1, 1201, 768]           1,536\n",
      "          Linear-276           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-277           [-1, 1201, 3072]               0\n",
      "         Dropout-278           [-1, 1201, 3072]               0\n",
      "          Linear-279            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-280            [-1, 1201, 768]               0\n",
      "             Mlp-281            [-1, 1201, 768]               0\n",
      "        Identity-282            [-1, 1201, 768]               0\n",
      "           Block-283            [-1, 1201, 768]               0\n",
      "       LayerNorm-284            [-1, 1201, 768]           1,536\n",
      "          Linear-285           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-286       [-1, 12, 1201, 1201]               0\n",
      "          Linear-287            [-1, 1201, 768]         590,592\n",
      "         Dropout-288            [-1, 1201, 768]               0\n",
      "       Attention-289            [-1, 1201, 768]               0\n",
      "        Identity-290            [-1, 1201, 768]               0\n",
      "       LayerNorm-291            [-1, 1201, 768]           1,536\n",
      "          Linear-292           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-293           [-1, 1201, 3072]               0\n",
      "         Dropout-294           [-1, 1201, 3072]               0\n",
      "          Linear-295            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-296            [-1, 1201, 768]               0\n",
      "             Mlp-297            [-1, 1201, 768]               0\n",
      "        Identity-298            [-1, 1201, 768]               0\n",
      "           Block-299            [-1, 1201, 768]               0\n",
      "       LayerNorm-300            [-1, 1201, 768]           1,536\n",
      "          Linear-301           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-302       [-1, 12, 1201, 1201]               0\n",
      "          Linear-303            [-1, 1201, 768]         590,592\n",
      "         Dropout-304            [-1, 1201, 768]               0\n",
      "       Attention-305            [-1, 1201, 768]               0\n",
      "        Identity-306            [-1, 1201, 768]               0\n",
      "       LayerNorm-307            [-1, 1201, 768]           1,536\n",
      "          Linear-308           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-309           [-1, 1201, 3072]               0\n",
      "         Dropout-310           [-1, 1201, 3072]               0\n",
      "          Linear-311            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-312            [-1, 1201, 768]               0\n",
      "             Mlp-313            [-1, 1201, 768]               0\n",
      "        Identity-314            [-1, 1201, 768]               0\n",
      "           Block-315            [-1, 1201, 768]               0\n",
      "       LayerNorm-316            [-1, 1201, 768]           1,536\n",
      "          Linear-317           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-318       [-1, 12, 1201, 1201]               0\n",
      "          Linear-319            [-1, 1201, 768]         590,592\n",
      "         Dropout-320            [-1, 1201, 768]               0\n",
      "       Attention-321            [-1, 1201, 768]               0\n",
      "        Identity-322            [-1, 1201, 768]               0\n",
      "       LayerNorm-323            [-1, 1201, 768]           1,536\n",
      "          Linear-324           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-325           [-1, 1201, 3072]               0\n",
      "         Dropout-326           [-1, 1201, 3072]               0\n",
      "          Linear-327            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-328            [-1, 1201, 768]               0\n",
      "             Mlp-329            [-1, 1201, 768]               0\n",
      "        Identity-330            [-1, 1201, 768]               0\n",
      "           Block-331            [-1, 1201, 768]               0\n",
      "       LayerNorm-332            [-1, 1201, 768]           1,536\n",
      "          Linear-333           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-334       [-1, 12, 1201, 1201]               0\n",
      "          Linear-335            [-1, 1201, 768]         590,592\n",
      "         Dropout-336            [-1, 1201, 768]               0\n",
      "       Attention-337            [-1, 1201, 768]               0\n",
      "        Identity-338            [-1, 1201, 768]               0\n",
      "       LayerNorm-339            [-1, 1201, 768]           1,536\n",
      "          Linear-340           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-341           [-1, 1201, 3072]               0\n",
      "         Dropout-342           [-1, 1201, 3072]               0\n",
      "          Linear-343            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-344            [-1, 1201, 768]               0\n",
      "             Mlp-345            [-1, 1201, 768]               0\n",
      "        Identity-346            [-1, 1201, 768]               0\n",
      "           Block-347            [-1, 1201, 768]               0\n",
      "       LayerNorm-348            [-1, 1201, 768]           1,536\n",
      "          Linear-349           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-350       [-1, 12, 1201, 1201]               0\n",
      "          Linear-351            [-1, 1201, 768]         590,592\n",
      "         Dropout-352            [-1, 1201, 768]               0\n",
      "       Attention-353            [-1, 1201, 768]               0\n",
      "        Identity-354            [-1, 1201, 768]               0\n",
      "       LayerNorm-355            [-1, 1201, 768]           1,536\n",
      "          Linear-356           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-357           [-1, 1201, 3072]               0\n",
      "         Dropout-358           [-1, 1201, 3072]               0\n",
      "          Linear-359            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-360            [-1, 1201, 768]               0\n",
      "             Mlp-361            [-1, 1201, 768]               0\n",
      "        Identity-362            [-1, 1201, 768]               0\n",
      "           Block-363            [-1, 1201, 768]               0\n",
      "       LayerNorm-364            [-1, 1201, 768]           1,536\n",
      "          Linear-365           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-366       [-1, 12, 1201, 1201]               0\n",
      "          Linear-367            [-1, 1201, 768]         590,592\n",
      "         Dropout-368            [-1, 1201, 768]               0\n",
      "       Attention-369            [-1, 1201, 768]               0\n",
      "        Identity-370            [-1, 1201, 768]               0\n",
      "       LayerNorm-371            [-1, 1201, 768]           1,536\n",
      "          Linear-372           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-373           [-1, 1201, 3072]               0\n",
      "         Dropout-374           [-1, 1201, 3072]               0\n",
      "          Linear-375            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-376            [-1, 1201, 768]               0\n",
      "             Mlp-377            [-1, 1201, 768]               0\n",
      "        Identity-378            [-1, 1201, 768]               0\n",
      "           Block-379            [-1, 1201, 768]               0\n",
      "       LayerNorm-380            [-1, 1201, 768]           1,536\n",
      "          Linear-381           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-382       [-1, 12, 1201, 1201]               0\n",
      "          Linear-383            [-1, 1201, 768]         590,592\n",
      "         Dropout-384            [-1, 1201, 768]               0\n",
      "       Attention-385            [-1, 1201, 768]               0\n",
      "        Identity-386            [-1, 1201, 768]               0\n",
      "       LayerNorm-387            [-1, 1201, 768]           1,536\n",
      "          Linear-388           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-389           [-1, 1201, 3072]               0\n",
      "         Dropout-390           [-1, 1201, 3072]               0\n",
      "          Linear-391            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-392            [-1, 1201, 768]               0\n",
      "             Mlp-393            [-1, 1201, 768]               0\n",
      "        Identity-394            [-1, 1201, 768]               0\n",
      "           Block-395            [-1, 1201, 768]               0\n",
      "       LayerNorm-396            [-1, 1201, 768]           1,536\n",
      "          Linear-397           [-1, 1201, 2304]       1,771,776\n",
      "         Dropout-398       [-1, 12, 1201, 1201]               0\n",
      "          Linear-399            [-1, 1201, 768]         590,592\n",
      "         Dropout-400            [-1, 1201, 768]               0\n",
      "       Attention-401            [-1, 1201, 768]               0\n",
      "        Identity-402            [-1, 1201, 768]               0\n",
      "       LayerNorm-403            [-1, 1201, 768]           1,536\n",
      "          Linear-404           [-1, 1201, 3072]       2,362,368\n",
      "            GELU-405           [-1, 1201, 3072]               0\n",
      "         Dropout-406           [-1, 1201, 3072]               0\n",
      "          Linear-407            [-1, 1201, 768]       2,360,064\n",
      "         Dropout-408            [-1, 1201, 768]               0\n",
      "             Mlp-409            [-1, 1201, 768]               0\n",
      "        Identity-410            [-1, 1201, 768]               0\n",
      "           Block-411            [-1, 1201, 768]               0\n",
      "       LayerNorm-412            [-1, 1201, 768]           1,536\n",
      "        Identity-413        [-1, 256, 120, 160]               0\n",
      "        Identity-414        [-1, 256, 120, 160]               0\n",
      "        Identity-415          [-1, 512, 60, 80]               0\n",
      "        Identity-416          [-1, 512, 60, 80]               0\n",
      "          Linear-417            [-1, 1200, 768]       1,180,416\n",
      "            GELU-418            [-1, 1200, 768]               0\n",
      "  ProjectReadout-419            [-1, 1200, 768]               0\n",
      "       Transpose-420            [-1, 768, 1200]               0\n",
      "          Linear-421            [-1, 1200, 768]       1,180,416\n",
      "            GELU-422            [-1, 1200, 768]               0\n",
      "  ProjectReadout-423            [-1, 1200, 768]               0\n",
      "       Transpose-424            [-1, 768, 1200]               0\n",
      "          Conv2d-425          [-1, 768, 30, 40]         590,592\n",
      "          Conv2d-426          [-1, 768, 30, 40]         590,592\n",
      "          Conv2d-427          [-1, 768, 15, 20]       5,309,184\n",
      "          Conv2d-428        [-1, 256, 120, 160]         589,824\n",
      "          Conv2d-429          [-1, 256, 60, 80]       1,179,648\n",
      "          Conv2d-430          [-1, 256, 30, 40]       1,769,472\n",
      "          Conv2d-431          [-1, 256, 15, 20]       1,769,472\n",
      "            ReLU-432          [-1, 256, 15, 20]               0\n",
      "            ReLU-433          [-1, 256, 15, 20]               0\n",
      "          Conv2d-434          [-1, 256, 15, 20]         590,080\n",
      "            ReLU-435          [-1, 256, 15, 20]               0\n",
      "            ReLU-436          [-1, 256, 15, 20]               0\n",
      "          Conv2d-437          [-1, 256, 15, 20]         590,080\n",
      "        Identity-438          [-1, 256, 15, 20]               0\n",
      "ResidualConvUnit_custom-439          [-1, 256, 15, 20]               0\n",
      "          Conv2d-440          [-1, 256, 30, 40]          65,792\n",
      "FeatureFusionBlock_custom-441          [-1, 256, 30, 40]               0\n",
      "            ReLU-442          [-1, 256, 30, 40]               0\n",
      "            ReLU-443          [-1, 256, 30, 40]               0\n",
      "          Conv2d-444          [-1, 256, 30, 40]         590,080\n",
      "            ReLU-445          [-1, 256, 30, 40]               0\n",
      "            ReLU-446          [-1, 256, 30, 40]               0\n",
      "          Conv2d-447          [-1, 256, 30, 40]         590,080\n",
      "        Identity-448          [-1, 256, 30, 40]               0\n",
      "ResidualConvUnit_custom-449          [-1, 256, 30, 40]               0\n",
      "        Identity-450          [-1, 256, 30, 40]               0\n",
      "            ReLU-451          [-1, 256, 30, 40]               0\n",
      "            ReLU-452          [-1, 256, 30, 40]               0\n",
      "          Conv2d-453          [-1, 256, 30, 40]         590,080\n",
      "            ReLU-454          [-1, 256, 30, 40]               0\n",
      "            ReLU-455          [-1, 256, 30, 40]               0\n",
      "          Conv2d-456          [-1, 256, 30, 40]         590,080\n",
      "        Identity-457          [-1, 256, 30, 40]               0\n",
      "ResidualConvUnit_custom-458          [-1, 256, 30, 40]               0\n",
      "          Conv2d-459          [-1, 256, 60, 80]          65,792\n",
      "FeatureFusionBlock_custom-460          [-1, 256, 60, 80]               0\n",
      "            ReLU-461          [-1, 256, 60, 80]               0\n",
      "            ReLU-462          [-1, 256, 60, 80]               0\n",
      "          Conv2d-463          [-1, 256, 60, 80]         590,080\n",
      "            ReLU-464          [-1, 256, 60, 80]               0\n",
      "            ReLU-465          [-1, 256, 60, 80]               0\n",
      "          Conv2d-466          [-1, 256, 60, 80]         590,080\n",
      "        Identity-467          [-1, 256, 60, 80]               0\n",
      "ResidualConvUnit_custom-468          [-1, 256, 60, 80]               0\n",
      "        Identity-469          [-1, 256, 60, 80]               0\n",
      "            ReLU-470          [-1, 256, 60, 80]               0\n",
      "            ReLU-471          [-1, 256, 60, 80]               0\n",
      "          Conv2d-472          [-1, 256, 60, 80]         590,080\n",
      "            ReLU-473          [-1, 256, 60, 80]               0\n",
      "            ReLU-474          [-1, 256, 60, 80]               0\n",
      "          Conv2d-475          [-1, 256, 60, 80]         590,080\n",
      "        Identity-476          [-1, 256, 60, 80]               0\n",
      "ResidualConvUnit_custom-477          [-1, 256, 60, 80]               0\n",
      "          Conv2d-478        [-1, 256, 120, 160]          65,792\n",
      "FeatureFusionBlock_custom-479        [-1, 256, 120, 160]               0\n",
      "            ReLU-480        [-1, 256, 120, 160]               0\n",
      "            ReLU-481        [-1, 256, 120, 160]               0\n",
      "          Conv2d-482        [-1, 256, 120, 160]         590,080\n",
      "            ReLU-483        [-1, 256, 120, 160]               0\n",
      "            ReLU-484        [-1, 256, 120, 160]               0\n",
      "          Conv2d-485        [-1, 256, 120, 160]         590,080\n",
      "        Identity-486        [-1, 256, 120, 160]               0\n",
      "ResidualConvUnit_custom-487        [-1, 256, 120, 160]               0\n",
      "        Identity-488        [-1, 256, 120, 160]               0\n",
      "            ReLU-489        [-1, 256, 120, 160]               0\n",
      "            ReLU-490        [-1, 256, 120, 160]               0\n",
      "          Conv2d-491        [-1, 256, 120, 160]         590,080\n",
      "            ReLU-492        [-1, 256, 120, 160]               0\n",
      "            ReLU-493        [-1, 256, 120, 160]               0\n",
      "          Conv2d-494        [-1, 256, 120, 160]         590,080\n",
      "        Identity-495        [-1, 256, 120, 160]               0\n",
      "ResidualConvUnit_custom-496        [-1, 256, 120, 160]               0\n",
      "          Conv2d-497        [-1, 256, 240, 320]          65,792\n",
      "FeatureFusionBlock_custom-498        [-1, 256, 240, 320]               0\n",
      "          Conv2d-499        [-1, 128, 240, 320]         295,040\n",
      "     Interpolate-500        [-1, 128, 480, 640]               0\n",
      "          Conv2d-501         [-1, 32, 480, 640]          36,896\n",
      "            ReLU-502         [-1, 32, 480, 640]               0\n",
      "          Conv2d-503          [-1, 1, 480, 640]              33\n",
      "            ReLU-504          [-1, 1, 480, 640]               0\n",
      "        Identity-505          [-1, 1, 480, 640]               0\n",
      "================================================================\n",
      "Total params: 120,753,921\n",
      "Trainable params: 120,753,921\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.52\n",
      "Forward/backward pass size (MB): 8416.20\n",
      "Params size (MB): 460.64\n",
      "Estimated Total Size (MB): 8880.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,480,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "117d76f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.353320533537529"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(48000 - 460.64) / 8880.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4812ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3FO4IW2QC9U7_original_1_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
